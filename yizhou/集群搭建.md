

# 0 Pre

使用 docker swarm 的方案搭建服务器集群

参考文章: https://www.cnblogs.com/zhujingzhi/p/9792432.html

`192.168.2.221`  主机名: manager221 担任角色：swarm manager

`192.168.2.222` 主机名: node222 担任角色：swarm node

`192.168.2.223` 主机名: node223 担任角色：swarm node

三台机器需安装好 docker

# 1 准备工作

**修改主机名:**

```shell
#在 221 执行
hostnamectl set-hostname manager221 
#在 222 执行
hostnamectl set-hostname node222 
#在 223 执行
hostnamectl set-hostname node223
```



**改 host 文件**

```shell
[root@localhost ~]# vi /etc/hosts
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6

192.168.2.221 manager221
192.168.2.222 node222
192.168.2.223 node223
```

```shell
# 复制文件到其余两个服务器
scp /etc/hosts root@192.168.2.222:/etc/hosts
scp /etc/hosts root@192.168.2.223:/etc/hosts
```



**设置防火墙**

关闭三台机器上的防火墙。如果开启防火墙，则需要在所有节点的防火墙上依次放行2377/tcp（管理端口）、7946/udp（节点间通信端口）、4789/udp（overlay 网络端口）端口。

```she
firewall-cmd --zone=public --add-port=2377/tcp --permanent
firewall-cmd --zone=public --add-port=7946/udp --permanent
firewall-cmd --zone=public --add-port=4789/udp --permanent
firewall-cmd --zone=public --add-port=7946/tcp --permanent
firewall-cmd --zone=public --add-port=4789/tcp --permanent
systemctl restart firewalld.service;
firewall-cmd --reload
```



# 2 创建 Swarm 并添加节点

**创建 Swarm 集群**

```shell
[192.168.2.221] docker swarm init --advertise-addr 192.168.2.221
#上面命令执行后，该机器自动加入到swarm集群。这个会创建一个集群token，获取全球唯一的 token，作为集群唯一标识。后续将其他节点加入集群都会用到这个token值。
#其中，--advertise-addr参数表示其它swarm中的worker节点使用此ip地址与manager联系。命令的输出包含了其它节点如何加入集群的命令。
```

```shell
[192.168.2.221] docker node ls
#查看集群中的机器
```

其他两台机器加入集群. 具体的命令是根据 ` docker swarm init --advertise-addr 192.168.2.221` 的返回

```shell
[192.168.2.222] docker swarm join --token SWMTKN-1-4uytr92lzo6a2gp1fvb538bdtfe3zrbdmqwqqr4yumjabtjfny-dnuldb1yn607by6zg41abdqya 192.168.2.221:2377
```

```shell
[192.168.2.223] docker swarm join --token SWMTKN-1-4uytr92lzo6a2gp1fvb538bdtfe3zrbdmqwqqr4yumjabtjfny-dnuldb1yn607by6zg41abdqya 192.168.2.221:2377
```



**将节点下线**

swarm 集群中 node 的 availability 状态可以为 active 或者 drain，其中：
active 状态下，node 可以接受来自 manager 节点的任务分派；
drain 状态下，node 节点会结束 task ，且不再接受来自 manager 节点的任务分派（也就是下线节点）

```shell
[192.168.2.221] docker node update --availability drain node222          
# 将node139节点下线。如果要删除node139节点，命令是"docker node rm --force node139"
```

**将节点上线**

```shell
[192.168.2.221] docker node update --availability active node222
```



**设置备用 manger** 

```shell
docker node promote 工作节点主机名1
docker node promote 工作节点主机名2 
```

当 manager 宕机时 会自动选举备用 manger



**以上工作已经实现docker swarm 集群**

---

# 3 部署 visualizer，portainer

这两个是 GUI 管理界面 更加方便监控 docker 中运行的容器状态

**使用 docker-compose.yml 的方式部署**

```yml
version: "3"
services:
  visualizer:
    image: dockersamples/visualizer
    ports:
      - "8080:8080"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
 
  portainer:
    image: portainer/portainer
    ports:
      - "9000:9000"
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
```



**运行 docker-compose.yml 文件**

```shell
docker stack deploy -c docker-compose.yml deploy_deamon
# 通过上面的执行过程可以看出这样创建会默认创建一个网络并使用它，名字都是我们给的名字的前缀加上服务名
```



**查看**

```shell
[root@manager221 ~]# docker service ls
ID                  NAME                       MODE                REPLICAS            IMAGE                             PORTS
2wkhwabzgm6b        deploy_deamon_portainer    replicated          1/1                 portainer/portainer:latest        *:9000->9000/tcp
o3zzwi542hr2        deploy_deamon_visualizer   replicated          1/1                 dockersamples/visualizer:latest   *:8080->8080/tcp

```







# 4 部署 Redis 主从

**确保已创建 overlay 网络**

```shell
[root@manager221 ~]# docker network ls
NETWORK ID          NAME                    DRIVER              SCOPE
4gkaxzhxeoq2        yizhou_net              overlay             swarm
```



**创建 Redis 实例并使用 overlay 网络模式**

```shell
[root@manager221 ~]# docker service create --replicas 3 --network yizhou_net --name redis -p 6379:6379 redis
```

以上创建了三个 redis 实例。它们会平均分布到集群中的节点中



**查看已创建的 redis 服务**

```shell
[root@manager221 ~]# docker service ps redis
ID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE               ERROR               PORTS
h81oy61bp0sh        redis.1             redis:latest        manager221          Running             Running about an hour ago                       
c1m3zqmyvokn        redis.2             redis:latest        node222             Running             Running about an hour ago                       
hhbc9fmj2ab9        redis.3             redis:latest        node223             Running             Running 27 minutes ago    
```

注: 如果某个服务器宕机了，那么 服务器的 redis 会自动转移到其他集群节点运行



**查看 master 节点上分配的 overlay 网络 ip**

```shell
[root@manager221 ~]# docker network inspect yizhou_net
   "Containers": {
            "4d0fb8d06694a4ae2bb10fcd63d1ca45fb954098fb0373340b93a768e05dca41": {
                "Name": "redis.1.h81oy61bp0sh8p6hxyx9suhma",
                "EndpointID": "393c376e7152d75bc08e239a2e0fdf16d2966386f95b837b40a5486d97a8d0e8",
                "MacAddress": "02:42:0a:00:03:0f",
                "IPv4Address": "10.0.3.15/24",
                "IPv6Address": ""
            }
```

由上可知 master 节点上的 ip 是 `10.0.3.15`



**在 node 节点服务器上进入 Redis 容器中**

```shell
docker ps #查看容器 redis 是否运行
docker exec -it xxxx /bin/bash # 进入 redis容器
redis-cli # 连接 redis 终端
slaveof 10.0.3.15 6379# 快捷方式设置主服务器 当前节点变为 从节点
docker logs -f xxx # 退出容器后 执行 查看日志 是否成功连接到 master 节点
```

注: 如果日志一直抛出无法连接 master 节点 , 可尝试 吧 master 和 当前服务器的防火墙关闭.再次尝试

`slaveof no one`  可使从节点退出主从模式



**验证**

```shell
#在 master 节点下的 redis 容器中
info replication # 查看是否有两个slave
set h 1 #设置一个值
#到 node 节点下 进入redis
get h # 查看能否获取值
```

